{"questions": ["What is the purpose of adopting ISO/IEC 27001 according to the document?", "How does ISO/IEC 27001 certification contribute to improving information security?", "What are the advantages of implementing ISO/IEC 27001 for an organization like FOSIR?", "How does ISO/IEC 27001 certification enhance customer and partner trust?"], "answers": ["I don't know.", "ISO/IEC 27001 certification contributes to improving information security by providing a framework for establishing, implementing, maintaining, and continually improving an information security management system. It helps organizations manage and protect their information assets effectively, ensuring confidentiality, integrity, and availability. This certification also demonstrates a commitment to information security best practices to stakeholders and customers.", "I don't know.", "ISO/IEC 27001 certification enhances customer and partner trust by demonstrating compliance with international standards for information security. This certification shows that the organization has implemented robust security measures to protect sensitive data, increasing confidence in the security of their services. Customers and partners can trust that their information is handled securely and that the organization takes data protection seriously."], "contexts": [["42. CSQA [254]: The CommonsenseQA is a question-\nanswering dataset that requires commonsense knowledge to\nanswer the ability of AI models to understand and answer\nquestions that require commonsense reasoning.\n43. GLUE [222]: The General Language Understanding\nEvaluation (GLUE) benchmark is a collection of resources\nfor training, evaluating, and analyzing natural language under-\nstanding systems. It includes a variety of tasks that test a wide\nrange of linguistic phenomena, making it a comprehensive tool\nfor evaluating language understanding in AI.\nVII. S UMMARY AND DISCUSSION\nA. Architecture\nDue to the gigantic scale of LLMs, minor changes\nin architecture and training strategies have a big impact\non performance and stability. Here, we summarize keyarchitectural modules used in various LLMs, leading to better\nperformance, reduced training time and memory, and better\ntraining stability.\nLayer Normalization is found to have a significant effect on", "\u2022To aid the model in effectively filtering and utilizing relevant information, human labelers play a crucial role in answering\nquestions regarding the usefulness of the retrieved documents.\n\u2022Interacting a fine-tuned language model with a text-based web-browsing environment can improve end-to-end retrieval and\nsynthesis via imitation learning and reinforcement learning.\n\u2022Generating answers with references can make labelers easily judge the factual accuracy of answers.\nTk-INSTRUCT\u2022Instruction tuning leads to a stronger generalization of unseen tasks\n\u2022More tasks improve generalization whereas only increasing task instances does not help\n\u2022Supervised trained models are better than generalized models\n\u2022Models pre-trained with instructions and examples perform well for different types of inputs\nmT0 and BLOOMZ\u2022Instruction tuning enables zero-shot generalization to the tasks never seen before\n\u2022Multi-lingual training leads to even better zero-shot generalization for both English and non-English", "[33], [32]. Infrequently, we also loosely follow the existing\nterminologies to ensure providing a more standardized outlook\nof this research direction. For instance, following [33], our\nsurvey considers a language model to be large if it has 10B\nparameters or more. Hence, we discuss such models in detail\nin this survey. We refer the readers interested in smaller models\nto [35], [36], [32].\nThe organization of this paper is as follows. Section II dis-\ncusses the background of LLMs. Section III focuses on LLMs\noverview, architectures, and training pipelines and strategies.\nSection IV presents the key findings derived from each LLM.\nSection V highlights the configuration and parameters that\nplay a crucial role in the functioning of these models. The\nLLM training and evaluation benchmarks are discussed in sec-\ntion VI, followed by concluding remarks and future directionJOURNAL OF L ATEX 4\nin the conclusion section.\nII. B ACKGROUND\nWe provide the relevant background to understand the key", "ings of the 27th international conference on computational linguistics ,\n2018, pp. 1952\u20131962. 21, 23\n[243] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\nC. Burns, S. Puranik, H. He, D. Song et al. , \u201cMeasuring coding\nchallenge competence with apps,\u201d arXiv preprint arXiv:2105.09938 ,\n2021. 21, 23, 24\n[244] I. Mollas, Z. Chrysopoulou, S. Karlos, and G. Tsoumakas, \u201cEthos: an\nonline hate speech detection dataset,\u201d arXiv preprint arXiv:2006.08328 ,\n2020. 21, 23, 24\n[245] M. Nadeem, A. Bethke, and S. Reddy, \u201cStereoset: Measuring\nstereotypical bias in pretrained language models,\u201d arXiv preprint\narXiv:2004.09456 , 2020. 21, 23, 24\n[246] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,\nH. Edwards, Y . Burda, N. Joseph, G. Brockman et al. , \u201cEvaluating large\nlanguage models trained on code,\u201d arXiv preprint arXiv:2107.03374 ,\n2021. 21, 23, 24, 25\n[247] Y . Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, and Y . Bisk, \u201cWe-"], ["\u2022To aid the model in effectively filtering and utilizing relevant information, human labelers play a crucial role in answering\nquestions regarding the usefulness of the retrieved documents.\n\u2022Interacting a fine-tuned language model with a text-based web-browsing environment can improve end-to-end retrieval and\nsynthesis via imitation learning and reinforcement learning.\n\u2022Generating answers with references can make labelers easily judge the factual accuracy of answers.\nTk-INSTRUCT\u2022Instruction tuning leads to a stronger generalization of unseen tasks\n\u2022More tasks improve generalization whereas only increasing task instances does not help\n\u2022Supervised trained models are better than generalized models\n\u2022Models pre-trained with instructions and examples perform well for different types of inputs\nmT0 and BLOOMZ\u2022Instruction tuning enables zero-shot generalization to the tasks never seen before\n\u2022Multi-lingual training leads to even better zero-shot generalization for both English and non-English", "42. CSQA [254]: The CommonsenseQA is a question-\nanswering dataset that requires commonsense knowledge to\nanswer the ability of AI models to understand and answer\nquestions that require commonsense reasoning.\n43. GLUE [222]: The General Language Understanding\nEvaluation (GLUE) benchmark is a collection of resources\nfor training, evaluating, and analyzing natural language under-\nstanding systems. It includes a variety of tasks that test a wide\nrange of linguistic phenomena, making it a comprehensive tool\nfor evaluating language understanding in AI.\nVII. S UMMARY AND DISCUSSION\nA. Architecture\nDue to the gigantic scale of LLMs, minor changes\nin architecture and training strategies have a big impact\non performance and stability. Here, we summarize keyarchitectural modules used in various LLMs, leading to better\nperformance, reduced training time and memory, and better\ntraining stability.\nLayer Normalization is found to have a significant effect on", "tasks have no effect\n\u2022Including small amounts i.e. 5% of pretraining data during fine-tuning is effective\n\u2022Only 1% reasoning data improves the performance, adding more deteriorates performance\n\u2022Adding dialogue data makes the performance worse\nFlan\u2022Finetuning with CoT improves performance on held-out tasks\n\u2022Fine-tuning along with CoT data improves reasoning abilities\n\u2022CoT tuning improves zero-shot reasoning\n\u2022Performance improves with more tasks\n\u2022Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models\n\u2022Improving the model\u2019s performance with instruction tuning is compute-efficient\n\u2022Multitask prompting enables zero-shot generalization abilities in LLM\nSparrow\u2022The judgments of labelers and the alignments with defined rules can help the model generate better responses.\n\u2022Good dialogue goals can be broken down into detailed natural language rules for the agent and the raters.", "ings of the 27th international conference on computational linguistics ,\n2018, pp. 1952\u20131962. 21, 23\n[243] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\nC. Burns, S. Puranik, H. He, D. Song et al. , \u201cMeasuring coding\nchallenge competence with apps,\u201d arXiv preprint arXiv:2105.09938 ,\n2021. 21, 23, 24\n[244] I. Mollas, Z. Chrysopoulou, S. Karlos, and G. Tsoumakas, \u201cEthos: an\nonline hate speech detection dataset,\u201d arXiv preprint arXiv:2006.08328 ,\n2020. 21, 23, 24\n[245] M. Nadeem, A. Bethke, and S. Reddy, \u201cStereoset: Measuring\nstereotypical bias in pretrained language models,\u201d arXiv preprint\narXiv:2004.09456 , 2020. 21, 23, 24\n[246] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,\nH. Edwards, Y . Burda, N. Joseph, G. Brockman et al. , \u201cEvaluating large\nlanguage models trained on code,\u201d arXiv preprint arXiv:2107.03374 ,\n2021. 21, 23, 24, 25\n[247] Y . Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, and Y . Bisk, \u201cWe-"], ["tasks have no effect\n\u2022Including small amounts i.e. 5% of pretraining data during fine-tuning is effective\n\u2022Only 1% reasoning data improves the performance, adding more deteriorates performance\n\u2022Adding dialogue data makes the performance worse\nFlan\u2022Finetuning with CoT improves performance on held-out tasks\n\u2022Fine-tuning along with CoT data improves reasoning abilities\n\u2022CoT tuning improves zero-shot reasoning\n\u2022Performance improves with more tasks\n\u2022Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models\n\u2022Improving the model\u2019s performance with instruction tuning is compute-efficient\n\u2022Multitask prompting enables zero-shot generalization abilities in LLM\nSparrow\u2022The judgments of labelers and the alignments with defined rules can help the model generate better responses.\n\u2022Good dialogue goals can be broken down into detailed natural language rules for the agent and the raters.", "advantage of the powerful pretrained model.\nJurassic-1\u2022The performance of an LLM is highly related to the network size.\n\u2022To improve runtime performance, more operations can be performed in parallel (width) rather than sequentially (depth).\n\u2022To efficiently represent and fit more text in the same context length, the model uses a larger vocabulary to train a SentencePiece\ntokenizer without restricting it to word boundaries. This tokenizer improvement can further benefit few-shot learning tasks.\nHyperCLOV A\u2022By employing prompt-based tuning, the performances of models can be improved, often surpassing those of state-of-the-art\nmodels when the backward gradients of inputs are accessible.\nYuan 1.0\u2022The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting behavior in zero-shot and\nfew-shot learning.\nGopher\u2022Relative encodings enable models to be evaluated for longer sequences than those on which it was trained.", "techniques. To speed up the training of Yuan 1.0 with the\naim of saving energy expenses and carbon emissions, various\nfactors that improve the performance of distributed training\nare incorporated in architecture and training like increasing\nthe number of hidden size improves pipeline and tensor par-\nallelism performance, larger micro batches improve pipeline\nparallelism performance, and higher global batch size improve\ndata parallelism performance. In practice, the Yuan 1.0 model\nperforms well on text classification, Winograd Schema, natural\nlanguage inference, and reading comprehension tasks.\n1.10 Gopher [98]: The Gopher family of models ranges\nfrom 44M to 280B parameters in size to study the effect of\nscale on the LLMs performance. The 280B model beats GPT-\n3 [8], Jurrasic-1 [94], MT-NLG [21], and others on 81% of\nthe evaluated tasks.\n1.11 ERNIE 3.0 TITAN [99]: ERNIE 3.0 Titan extends\nERNIE 3.0 by training a larger model with 26x the number of", "\u2022To aid the model in effectively filtering and utilizing relevant information, human labelers play a crucial role in answering\nquestions regarding the usefulness of the retrieved documents.\n\u2022Interacting a fine-tuned language model with a text-based web-browsing environment can improve end-to-end retrieval and\nsynthesis via imitation learning and reinforcement learning.\n\u2022Generating answers with references can make labelers easily judge the factual accuracy of answers.\nTk-INSTRUCT\u2022Instruction tuning leads to a stronger generalization of unseen tasks\n\u2022More tasks improve generalization whereas only increasing task instances does not help\n\u2022Supervised trained models are better than generalized models\n\u2022Models pre-trained with instructions and examples perform well for different types of inputs\nmT0 and BLOOMZ\u2022Instruction tuning enables zero-shot generalization to the tasks never seen before\n\u2022Multi-lingual training leads to even better zero-shot generalization for both English and non-English"], ["\u2022To aid the model in effectively filtering and utilizing relevant information, human labelers play a crucial role in answering\nquestions regarding the usefulness of the retrieved documents.\n\u2022Interacting a fine-tuned language model with a text-based web-browsing environment can improve end-to-end retrieval and\nsynthesis via imitation learning and reinforcement learning.\n\u2022Generating answers with references can make labelers easily judge the factual accuracy of answers.\nTk-INSTRUCT\u2022Instruction tuning leads to a stronger generalization of unseen tasks\n\u2022More tasks improve generalization whereas only increasing task instances does not help\n\u2022Supervised trained models are better than generalized models\n\u2022Models pre-trained with instructions and examples perform well for different types of inputs\nmT0 and BLOOMZ\u2022Instruction tuning enables zero-shot generalization to the tasks never seen before\n\u2022Multi-lingual training leads to even better zero-shot generalization for both English and non-English", "tasks have no effect\n\u2022Including small amounts i.e. 5% of pretraining data during fine-tuning is effective\n\u2022Only 1% reasoning data improves the performance, adding more deteriorates performance\n\u2022Adding dialogue data makes the performance worse\nFlan\u2022Finetuning with CoT improves performance on held-out tasks\n\u2022Fine-tuning along with CoT data improves reasoning abilities\n\u2022CoT tuning improves zero-shot reasoning\n\u2022Performance improves with more tasks\n\u2022Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models\n\u2022Improving the model\u2019s performance with instruction tuning is compute-efficient\n\u2022Multitask prompting enables zero-shot generalization abilities in LLM\nSparrow\u2022The judgments of labelers and the alignments with defined rules can help the model generate better responses.\n\u2022Good dialogue goals can be broken down into detailed natural language rules for the agent and the raters.", "42. CSQA [254]: The CommonsenseQA is a question-\nanswering dataset that requires commonsense knowledge to\nanswer the ability of AI models to understand and answer\nquestions that require commonsense reasoning.\n43. GLUE [222]: The General Language Understanding\nEvaluation (GLUE) benchmark is a collection of resources\nfor training, evaluating, and analyzing natural language under-\nstanding systems. It includes a variety of tasks that test a wide\nrange of linguistic phenomena, making it a comprehensive tool\nfor evaluating language understanding in AI.\nVII. S UMMARY AND DISCUSSION\nA. Architecture\nDue to the gigantic scale of LLMs, minor changes\nin architecture and training strategies have a big impact\non performance and stability. Here, we summarize keyarchitectural modules used in various LLMs, leading to better\nperformance, reduced training time and memory, and better\ntraining stability.\nLayer Normalization is found to have a significant effect on", "UL2\u2022Mode switching training enables better performance on downstream tasks\n\u2022CoT prompting outperforms standard prompting for UL2\nGLM-130B\u2022Pre-training data with a small proportion of multi-task instruction data improves the overall model performance\nCodeGen\u2022Multi-step prompting for code synthesis leads to a better user intent understanding and code generation\nLLaMA\u2022LLaMA is open-source and can be fine-tuned or continually pre-trained to develop new models or instruction-based tools.\n\u2022A few optimizations are proposed to improve the training efficiency of LLaMA, such as efficient implementation of multi-head\nself-attention and a reduced amount of activations during back-propagation.\n\u2022Training exclusively on public data can also achieve state-of-the-art performance.\n\u2022A constant performance improvement is gained when scaling the model.\n\u2022Smaller models can also realize good performances using more training data and time."]]}